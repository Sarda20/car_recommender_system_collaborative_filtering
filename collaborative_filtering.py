# -*- coding: utf-8 -*-
"""DS_Workflow_Collaboraive_filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13WJwbTys4KxE1MYQEdtN-yjAnliHM9qg

### Package Import
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

from surprise import Reader, BaselineOnly, KNNBasic, KNNBaseline, SVD, SVDpp, NMF
from surprise.model_selection import train_test_split
from surprise import accuracy, Prediction

import torch
import torch.nn as nn
import torch.optim as optim
from surprise import Dataset
#from torch.utils.data import DataLoader,
from torch.utils.data import DataLoader, TensorDataset # Import TensorDataset
import random

from scipy.sparse import csr_matrix
from lightfm import LightFM
from lightfm.evaluation import auc_score, precision_at_k

# !pip uninstall pandas matplotlib seaborn scikit-learn surprise
# !pip install pandas matplotlib seaborn scikit-learn scikit-surprise

# !pip install pandas==2.2.2
# !pip install scikit-surprise
# !pip install pandas==2.2.3 numpy==1.26.4 scikit-surprise==1.1.4 scikit-learn==1.5.2
!pip install lightfm

# Load the uploaded CSV sample
csv_path = "/mnt/car_reviews_full.csv"
df = pd.read_csv(csv_path)

# Get a preview and summary
df_preview = df.head()
df_info = df.info()
df_description = df.describe(include='all')

df_preview

"""### Creating the User Item Matrix"""

df = df.drop_duplicates()
df['Rating'] = df['Rating'].astype(float)
df['user_id'] = df['Reviewer'].astype('category').cat.codes
df['item_id'] = df['Car Model'].astype('category').cat.codes

n_users = df['user_id'].nunique()
n_items = df['item_id'].nunique()

user_item_matrix = np.zeros((n_users, n_items))
for row in df.itertuples():
    user_item_matrix[row.user_id, row.item_id] = row.Rating

user_item_matrix[:5]

"""### Train Test Spliting"""

train_matrix = user_item_matrix.copy()
mask = train_matrix > 0
train_data, test_data = train_test_split(np.argwhere(mask), test_size=0.2, random_state=42)
test_matrix = np.zeros_like(user_item_matrix)
for user, item in test_data:
    test_matrix[user, item] = user_item_matrix[user, item]
    train_matrix[user, item] = 0

"""### Using Truncated SVD package as per the class files"""

svd = TruncatedSVD(n_components=2)
svd_matrix = svd.fit_transform(train_matrix)
svd_pred = np.dot(svd_matrix, svd.components_)
svd_mse = mean_squared_error(test_matrix[test_matrix > 0], svd_pred[test_matrix > 0])
svd_rmse = np.sqrt(svd_mse)

print("SVD RMSE:", svd_rmse)
print("SVD MSE:", svd_mse)

"""### Using Surprise Package

#### SVD
"""

# Preprocessing your DataFrame (as you had it)
df['Rating'] = df['Rating'].astype(int)
df['user_id'] = df['Reviewer'].astype('category').cat.codes
df['item_id'] = df['Car Model'].astype('category').cat.codes

# Load data into Surprise format
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df[['user_id', 'item_id', 'Rating']], reader)

# Split data into train and test sets
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# 1. Create SVD model
model = SVD(random_state=42)

# 2. Fit the model instance to the training data
model.fit(trainset)

# Now you can make predictions or evaluate the model
predictions = model.test(testset)

# And evaluate accuracy:
rmse = accuracy.rmse(predictions, verbose=False)
mse = accuracy.mse(predictions, verbose=False)
mae = accuracy.mae(predictions, verbose=False)
fcp = accuracy.fcp(predictions, verbose=False)
metrics = {'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'FCP': fcp}
metrics

"""#### Comapring SVD SVD++ Baseline KNNBasic NMF"""

# Dictionary to store results
results = {}

# --- Define algorithms to test ---
algorithms = {
    'BaselineOnly': BaselineOnly(),
    'KNNBasic': KNNBasic(sim_options={'name': 'cosine', 'user_based': True}), # User-based KNN with cosine similarity
    'KNNBaseline': KNNBaseline(sim_options={'name': 'cosine', 'user_based': True}), # User-based KNN with baseline
    'SVD': SVD(random_state=42),
    'SVDpp': SVDpp(random_state=42),
    'NMF': NMF(random_state=42)
}

print("Starting algorithm training and evaluation...\n")

for algo_name, algo_instance in algorithms.items():
    print(f"--- Training and evaluating {algo_name} ---")
    # Train the algorithm
    algo_instance.fit(trainset)

    # Make predictions on the test set
    predictions = algo_instance.test(testset)

    # Calculate metrics
    rmse = accuracy.rmse(predictions, verbose=False)
    mse = accuracy.mse(predictions, verbose=False)
    mae = accuracy.mae(predictions, verbose=False)
    fcp = accuracy.fcp(predictions, verbose=False)

    # Store results
    results[algo_name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'FCP': fcp
    }
    print(f"{algo_name} done.\n")

# Convert results to a DataFrame for easy comparison
metrics_df = pd.DataFrame.from_dict(results, orient='index')

print("--- Comparison of Algorithm Metrics ---")
print(metrics_df)

print("\n--- Reasoning for Results ---")
print("RMSE, MSE, MAE: Lower values are better, indicating less prediction error.")
print("FCP (Fraction of Concordant Pairs): Higher values are better, indicating better ranking accuracy.")
print("\nGenerally, you'll observe:")
print("- **BaselineOnly:** Provides a simple baseline by predicting ratings based on overall average, user biases, and item biases. It's often surprisingly effective for its simplicity.")
print("- **KNNBasic:** Collaborative filtering based on item or user similarity. Performance depends heavily on the 'neighbors' and similarity measure. Can suffer from sparsity.")
print("- **KNNBaseline:** Improves KNNBasic by incorporating baselines (biases). Often performs better than KNNBasic as it accounts for individual user/item tendencies.")
print("- **SVD (Singular Value Decomposition):** A matrix factorization technique that decomposes the user-item interaction matrix into latent factors. Typically strong performer, especially for dense datasets.")
print("- **SVD++ (SVD with implicit feedback):** An extension of SVD that incorporates implicit feedback (e.g., users who have rated an item, even if the rating isn't explicitly used). Often outperforms SVD, especially when implicit feedback is available.")
print("- **NMF (Non-negative Matrix Factorization):** Another matrix factorization technique that decomposes the matrix into non-negative latent factors. Can be useful for interpretability but might not always be the top performer in terms of raw accuracy compared to SVD-based methods.")
print("\nIn most real-world scenarios, **SVD and SVD++ often yield the best RMSE/MAE results**, while KNN-based methods can be competitive depending on data characteristics, and `BaselineOnly` serves as a good simple benchmark.")
print("The specific performance will depend on the dataset's sparsity, density, and inherent patterns.")

"""#### For Sample User Prediction"""

print("\n--- Generating Sample Predictions for a User ---")

# Select a sample user from your original DataFrame
sample_reviewer = df['Reviewer'].iloc[0] # Get the name of the first reviewer

# Corrected logic for getting sample_user_id
if 'user_id' in df.columns and sample_reviewer in df['Reviewer'].unique():
    sample_user_id = df.loc[df['Reviewer'] == sample_reviewer, 'user_id'].iloc[0]
    print(f"Sample User: '{sample_reviewer}' (encoded as user_id: {sample_user_id})")
else:
    print(f"Error: Reviewer '{sample_reviewer}' not found or 'user_id' column is missing after processing.")
    sample_user_id = 0 # Using a dummy, adjust if critical for your data
    print(f"Using fallback sample_user_id: {sample_user_id}")


# Get items the user has NOT rated in the trainset
all_item_ids = df['item_id'].unique()

user_rated_items_trainset = set()
# Corrected line here:
for (item_inner_id, rating) in trainset.ur[sample_user_id]: # Unpack 2 values
    user_rated_items_trainset.add(item_inner_id)


unrated_item_ids = [iid for iid in all_item_ids if iid not in user_rated_items_trainset]

if len(unrated_item_ids) == 0:
    print("This user has rated all available items in the training set or no unrated items found for prediction.")
    print("Please select a user with unrated items or ensure your dataset is large enough.")
else:
    # Map encoded item_ids back to original 'Car Model' names for readability
    item_id_to_name = df.set_index('item_id')['Car Model'].drop_duplicates().to_dict()


    print(f"\nTop 5 predicted ratings for user '{sample_reviewer}' (using SVD model):")
    predictions_for_user = []
    for item_id_encoded in unrated_item_ids:
        predicted_rating = algorithms['SVD'].predict(sample_user_id, item_id_encoded).est
        predictions_for_user.append((item_id_to_name.get(item_id_encoded, f"Unknown Item ({item_id_encoded})"), predicted_rating))

    predictions_for_user.sort(key=lambda x: x[1], reverse=True)

    for item_name, pred_rating in predictions_for_user[:5]:
        print(f"  - Car Model: {item_name}, Predicted Rating: {pred_rating:.2f}")

    print("\nNote: These are predicted ratings for items the user has not interacted with in the training set.")

print("\n--- Generating Sample Predictions for a User Across All Methods ---")

# Select a sample user from your original DataFrame
sample_reviewer = df['Reviewer'].iloc[0] # Get the name of the first reviewer

# Get the encoded user_id
sample_user_id = df.loc[df['Reviewer'] == sample_reviewer, 'user_id'].iloc[0]
print(f"Sample User: '{sample_reviewer}' (encoded as user_id: {sample_user_id})")

# Get items the user has NOT rated in the trainset
all_item_ids = df['item_id'].unique()

user_rated_items_trainset = set()
for (item_inner_id, rating) in trainset.ur[sample_user_id]:
    user_rated_items_trainset.add(item_inner_id)

unrated_item_ids = [iid for iid in all_item_ids if iid not in user_rated_items_trainset]

# Map encoded item_ids back to original 'Car Model' names for readability
item_id_to_name = df.set_index('item_id')['Car Model'].drop_duplicates().to_dict()

if len(unrated_item_ids) == 0:
    print("This user has rated all available items in the training set or no unrated items found for prediction.")
    print("Please select a user with unrated items or ensure your dataset is large enough.")
elif len(unrated_item_ids) < 3: # Handle small number of unrated items
    print(f"Only {len(unrated_item_ids)} unrated items found for this user. Predicting for these items.")
    items_to_predict_for = unrated_item_ids
else:
    # For a clearer comparison, let's just pick the first few unrated items
    # or you could pick random ones, or the ones with highest overall popularity etc.
    items_to_predict_for = unrated_item_ids[:3] # Predict for the first 3 unrated items

if items_to_predict_for:
    # Prepare a DataFrame to store comparative predictions
    prediction_comparison_data = {}
    for item_id_encoded in items_to_predict_for:
        item_name = item_id_to_name.get(item_id_encoded, f"Unknown Item ({item_id_encoded})")
        prediction_comparison_data[item_name] = {}
        for algo_name, algo_instance in algorithms.items():
            predicted_rating = algo_instance.predict(sample_user_id, item_id_encoded).est
            prediction_comparison_data[item_name][algo_name] = round(predicted_rating, 2)

    prediction_comparison_df = pd.DataFrame.from_dict(prediction_comparison_data, orient='index')
    prediction_comparison_df.columns.name = 'Algorithm'
    prediction_comparison_df.index.name = 'Car Model'

    print(f"\n--- Predicted Ratings for User '{sample_reviewer}' by Algorithm ---")
    print(prediction_comparison_df)

    print("\nObservation:")
    print("This table shows the predicted rating for specific unrated 'Car Models' by each algorithm.")
    print("You might observe differences in how each algorithm predicts, reflecting their different underlying methodologies.")
    print("For instance, BaselineOnly might predict ratings closer to the overall average or user/item biases, while SVD/SVD++ might pick up on more subtle latent features.")
else:
    print("Could not generate sample predictions as there are no suitable unrated items for the chosen user.")

"""#### Inference"""

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Re-create the metrics DataFrame
data = {
    'RMSE': [0.853003, 0.891617, 0.875810, 0.853340, 0.857117, 0.916162],
    'MSE': [0.727615, 0.794981, 0.767043, 0.728189, 0.734649, 0.839353],
    'MAE': [0.549816, 0.584153, 0.557841, 0.549015, 0.567239, 0.622770],
    'FCP': [0.497258, 0.541547, 0.512821, 0.435484, 0.476008, 0.494071]
}
algorithms = ['BaselineOnly', 'KNNBasic', 'KNNBaseline', 'SVD', 'SVDpp', 'NMF']
metrics_df = pd.DataFrame(data, index=algorithms)
metrics_df.index.name = 'Algorithm'

# Create subplots: one for error metrics (RMSE/MAE) and one for FCP
fig = make_subplots(rows=1, cols=2,
                    subplot_titles=("Error Metrics (Lower is Better)", "Ranking Metric (FCP - Higher is Better)"))

# Plot Error Metrics (RMSE and MAE)
fig.add_trace(go.Bar(
    x=metrics_df.index,
    y=metrics_df['RMSE'],
    name='RMSE',
    marker_color='skyblue',
    hovertemplate='<b>%{x}</b><br>RMSE: %{y:.4f}<extra></extra>'
), row=1, col=1)

fig.add_trace(go.Bar(
    x=metrics_df.index,
    y=metrics_df['MAE'],
    name='MAE',
    marker_color='lightcoral',
    hovertemplate='<b>%{x}</b><br>MAE: %{y:.4f}<extra></extra>'
), row=1, col=1)

# Plot FCP
fig.add_trace(go.Bar(
    x=metrics_df.index,
    y=metrics_df['FCP'],
    name='FCP',
    marker_color='lightgreen',
    hovertemplate='<b>%{x}</b><br>FCP: %{y:.4f}<extra></extra>'
), row=1, col=2)

# Update layout
fig.update_layout(
    title_text='Comparison of Recommendation Algorithm Performance',
    height=500,
    showlegend=True,
    barmode='group', # Group bars for RMSE/MAE
    hovermode='x unified' # Show hover info for all bars at that x-position
)

fig.update_yaxes(title_text="Error Value", row=1, col=1)
fig.update_yaxes(title_text="FCP Value", row=1, col=2)

fig.show()

# Conclusion based on results
print("\n--- Algorithm Selection Reasoning ---")
print("Based on the provided metrics:")

best_rmse_algo = metrics_df['RMSE'].idxmin()
best_rmse_val = metrics_df['RMSE'].min()
best_mae_algo = metrics_df['MAE'].idxmin()
best_mae_val = metrics_df['MAE'].min()
best_fcp_algo = metrics_df['FCP'].idxmax()
best_fcp_val = metrics_df['FCP'].max()

print(f"\n- **Best for RMSE/MAE (Accuracy of Prediction):** '{best_rmse_algo}' (RMSE: {best_rmse_val:.4f}, MAE: {metrics_df.loc[best_rmse_algo, 'MAE']:.4f})")
print(f"  - This indicates that '{best_rmse_algo}' provides the most accurate point predictions for user ratings on average.")
print(f"- **Best for FCP (Ranking Accuracy):** '{best_fcp_algo}' (FCP: {best_fcp_val:.4f})")
print(f"  - This indicates that '{best_fcp_algo}' is best at predicting the relative order of preferences, which is crucial for recommendation lists.")

print("\n**Recommendation:**")
if best_rmse_algo == 'BaselineOnly' and best_fcp_algo == 'KNNBasic':
    print(f"Given that **BaselineOnly** achieved the **best RMSE/MAE** (meaning highly accurate predicted ratings) and is generally very fast and simple, it is a very strong contender for implementation.")
    print(f"However, if **ranking accuracy** is paramount (e.g., if you care more about showing the 'right' items at the top of a list even if the exact rating prediction isn't perfect), then **{best_fcp_algo}** might be preferred, despite its higher error metrics.")

"""### Autoencoder Decoder"""



"""#### Data Prep for the model"""

# Create a mapping for original IDs if needed for prediction display
reviewer_id_to_original = dict(enumerate(df['Reviewer'].astype('category').cat.categories))
item_id_to_original = dict(enumerate(df['Car Model'].astype('category').cat.categories))

# Convert to Surprise Dataset for splitting (to keep consistent with previous setup)
# Load data into Surprise format
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df[['user_id', 'item_id', 'Rating']], reader)

# Split data into train and test sets
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# Get the total number of users and items from the *full* dataset
# This is crucial for defining the input/output layer size of the autoencoder
n_users = df['user_id'].nunique()
n_items = df['item_id'].nunique()

print(f"Number of unique users: {n_users}")
print(f"Number of unique items: {n_items}")

# Create the user-item matrix for the autoencoder
# We'll build a sparse matrix for training input
# Initialize with zeros, then fill known ratings
train_matrix = torch.zeros((n_users, n_items))
for uid, iid, rating in trainset.all_ratings():
    train_matrix[uid, iid] = rating

# Create a mask for known ratings (where rating is > 0)
# This is used for the masked MSE loss
train_mask = (train_matrix > 0).float()

print(f"Shape of training matrix: {train_matrix.shape}")
print(f"Number of known ratings in training matrix: {train_mask.sum().item()}")

class Autoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
            nn.ReLU() # Use ReLU for non-negativity in latent space
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # Sigmoid activation to scale output between 0 and 1, then scale to rating_scale
            # Or use identity if you plan to clip/scale manually and want linear output
            nn.Sigmoid()
        )
        self.rating_scale_min = reader.rating_scale[0]
        self.rating_scale_max = reader.rating_scale[1]

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        # Scale the output to the original rating range (e.g., 1-5)
        scaled_decoded = decoded * (self.rating_scale_max - self.rating_scale_min) + self.rating_scale_min
        return scaled_decoded

#  Custom Loss Function (Masked MSE)
class MaskedMSELoss(nn.Module):
    def __init__(self):
        super(MaskedMSELoss, self).__init__()

    def forward(self, predictions, targets, mask):
        # Only consider positions where mask is 1 (i.e., known ratings)
        masked_predictions = predictions * mask
        masked_targets = targets * mask
        loss = (masked_predictions - masked_targets)**2
        # Sum of squared errors divided by the number of known ratings
        return loss.sum() / mask.sum()

"""#### Training with hyperparameters"""

# --- Hyperparameters ---
input_dim = n_items
hidden_dim = 128  # Can be tuned
latent_dim = 64   # Can be tuned, this is the bottleneck dimension
learning_rate = 0.001
num_epochs = 100 # Adjust based on dataset size and convergence
batch_size = 32  # For DataLoader, if you're using it (here we train on full matrix for simplicity)

# Instantiate model, loss, and optimizer
model = Autoencoder(input_dim, hidden_dim, latent_dim)
criterion = MaskedMSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
train_matrix = train_matrix.to(device)
train_mask = train_mask.to(device)

#  Training Loop ---
print("\n--- Training Autoencoder ---")
for epoch in range(num_epochs):
    model.train() # Set model to training mode
    optimizer.zero_grad() # Clear gradients

    # Forward pass
    outputs = model(train_matrix)

    # Calculate loss using the mask
    loss = criterion(outputs, train_matrix, train_mask)

    # Backward pass and optimize
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

print("\nAutoencoder training complete.")

#  Prediction and Evaluation ---

model.eval() # Set model to evaluation mode

# Get reconstructed matrix (contains predicted ratings for all items for all users)
with torch.no_grad(): # Disable gradient calculation for inference
    reconstructed_matrix = model(train_matrix).cpu().numpy() # Move back to CPU for numpy operations

# Convert Surprise testset to a format usable for evaluation
test_predictions = []
for uid, iid, true_r in testset:
    # Get predicted rating from the reconstructed matrix
    predicted_r = reconstructed_matrix[uid, iid]
    test_predictions.append(Prediction(uid, iid, true_r, predicted_r, None)) # Use Prediction directly

# Evaluate with Surprise's accuracy metrics
print("\n--- Autoencoder Model Evaluation ---")
ae_rmse = accuracy.rmse(test_predictions, verbose=False)
ae_mse = accuracy.mse(test_predictions, verbose=False)
ae_mae = accuracy.mae(test_predictions, verbose=False)
ae_fcp = accuracy.fcp(test_predictions, verbose=False)

print(f"Autoencoder RMSE: {ae_rmse:.4f}")
print(f"Autoencoder MSE: {ae_mse:.4f}")
print(f"Autoencoder MAE: {ae_mae:.4f}")
print(f"Autoencoder FCP: {ae_fcp:.4f}")

#  Add Autoencoder to Comparison Table and Plot ---

metrics_data_prev = {
    'RMSE': [0.853003, 0.891617, 0.875810, 0.853340, 0.857117, 0.916162],
    'MSE': [0.727615, 0.794981, 0.767043, 0.728189, 0.734649, 0.839353],
    'MAE': [0.549816, 0.584153, 0.557841, 0.549015, 0.567239, 0.622770],
    'FCP': [0.497258, 0.541547, 0.512821, 0.435484, 0.476008, 0.494071]
}
algorithms_prev = ['BaselineOnly', 'KNNBasic', 'KNNBaseline', 'SVD', 'SVDpp', 'NMF']
metrics_df = pd.DataFrame(metrics_data_prev, index=algorithms_prev)
metrics_df.index.name = 'Algorithm'


# Add Autoencoder results to the DataFrame
ae_results = {'RMSE': ae_rmse, 'MSE': ae_mse, 'MAE': ae_mae, 'FCP': ae_fcp}
metrics_df.loc['Autoencoder'] = ae_results

print("\n--- Updated Comparison of Algorithm Metrics (with Autoencoder) ---")
print(metrics_df)

# --- Plotly Chart (updated to include Autoencoder) ---
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=2,
                    subplot_titles=("Error Metrics (Lower is Better)", "Ranking Metric (FCP - Higher is Better)"))

fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['RMSE'], name='RMSE', marker_color='skyblue', hovertemplate='<b>%{x}</b><br>RMSE: %{y:.4f}<extra></extra>'), row=1, col=1)
fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['MAE'], name='MAE', marker_color='lightcoral', hovertemplate='<b>%{x}</b><br>MAE: %{y:.4f}<extra></extra>'), row=1, col=1)
fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['FCP'], name='FCP', marker_color='lightgreen', hovertemplate='<b>%{x}</b><br>FCP: %{y:.4f}<extra></extra>'), row=1, col=2)

fig.update_layout(
    title_text='Comparison of Recommendation Algorithm Performance',
    height=500,
    showlegend=True,
    barmode='group',
    hovermode='x unified'
)

fig.update_yaxes(title_text="Error Value", row=1, col=1)
fig.update_yaxes(title_text="FCP Value", row=1, col=2)

fig.show()

# --- Sample User Prediction with Autoencoder ---
print("\n--- Generating Sample Predictions for a User (Autoencoder) ---")

sample_reviewer_original = df['Reviewer'].iloc[0] # Using original ID for display
sample_user_id_encoded = df.loc[df['Reviewer'] == sample_reviewer_original, 'user_id'].iloc[0]

print(f"Sample User: '{sample_reviewer_original}' (encoded as user_id: {sample_user_id_encoded})")

# Get items the user has NOT rated in the trainset
all_item_ids = df['item_id'].unique()
user_rated_items_trainset = set()
for (item_inner_id, rating) in trainset.ur[sample_user_id_encoded]:
    user_rated_items_trainset.add(item_inner_id)

unrated_item_ids = [iid for iid in all_item_ids if iid not in user_rated_items_trainset]

item_id_to_name = df.set_index('item_id')['Car Model'].drop_duplicates().to_dict()

if len(unrated_item_ids) == 0:
    print("This user has rated all available items in the training set or no unrated items found for prediction.")
else:
    items_to_predict_for_ae = unrated_item_ids[:min(len(unrated_item_ids), 5)] # Predict for up to 5 unrated items

    if items_to_predict_for_ae:
        print(f"\nTop {len(items_to_predict_for_ae)} predicted ratings for user '{sample_reviewer_original}' (using Autoencoder model):")
        predictions_for_user_ae = []
        for item_id_encoded in items_to_predict_for_ae:
            # Predictions come directly from the reconstructed_matrix
            predicted_rating = reconstructed_matrix[sample_user_id_encoded, item_id_encoded]
            predictions_for_user_ae.append((item_id_to_name.get(item_id_encoded, f"Unknown Item ({item_id_encoded})"), predicted_rating))

        predictions_for_user_ae.sort(key=lambda x: x[1], reverse=True)

        for item_name, pred_rating in predictions_for_user_ae:
            print(f"  - Car Model: {item_name}, Predicted Rating: {pred_rating:.2f}")
    else:
        print("No unrated items available for prediction.")

"""#### Enhanced Autoencoder decoder"""

# Define the Enhanced Autoencoder Model ---

class EnhancedAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dims, latent_dim, dropout_rate=0.5):
        super(EnhancedAutoencoder, self).__init__()

        layers = []
        # Encoder
        current_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(current_dim, h_dim))
            layers.append(nn.LeakyReLU()) # Using LeakyReLU
            layers.append(nn.Dropout(dropout_rate)) # Dropout layer
            current_dim = h_dim
        layers.append(nn.Linear(current_dim, latent_dim))
        layers.append(nn.LeakyReLU()) # LeakyReLU before latent
        self.encoder = nn.Sequential(*layers)

        layers = []
        # Decoder (mirroring encoder, starting from latent_dim)
        current_dim = latent_dim
        for h_dim in reversed(hidden_dims):
            layers.append(nn.Linear(current_dim, h_dim))
            layers.append(nn.LeakyReLU()) # Using LeakyReLU
            layers.append(nn.Dropout(dropout_rate)) # Dropout layer
            current_dim = h_dim
        layers.append(nn.Linear(current_dim, input_dim))
        layers.append(nn.Sigmoid()) # Output activation for scaling

        self.decoder = nn.Sequential(*layers)

        # Store rating scale for output scaling
        self.rating_scale_min = reader.rating_scale[0]
        self.rating_scale_max = reader.rating_scale[1]

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        # Scale the output to the original rating range (e.g., 1-5)
        scaled_decoded = decoded * (self.rating_scale_max - self.rating_scale_min) + self.rating_scale_min
        return scaled_decoded

#  Custom Loss Function (Masked MSE) ---
class MaskedMSELoss(nn.Module):
    def __init__(self):
        super(MaskedMSELoss, self).__init__()

    def forward(self, predictions, targets, mask):
        # Only consider positions where mask is 1 (i.e., known ratings)
        masked_predictions = predictions * mask
        masked_targets = targets * mask
        loss = (masked_predictions - masked_targets)**2
        # Sum of squared errors divided by the number of known ratings
        # Add a small epsilon to the denominator to prevent division by zero if mask.sum() is 0
        return loss.sum() / (mask.sum() + 1e-8)


# --- Hyperparameters ---
input_dim = n_items
hidden_dims = [256, 128] # Deeper architecture
latent_dim = 64
dropout_rate = 0.3 # Dropout rate
learning_rate = 0.001
num_epochs = 200 # Increased epochs for potentially deeper model
weight_decay = 1e-5 # L2 regularization (weight decay)

# This dataset yields (user_vector, mask_vector) for each user
train_dataset = TensorDataset(train_matrix, train_mask)
batch_size = 32 # Define batch size
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Instantiate model, loss, and optimizer
model = EnhancedAutoencoder(input_dim, hidden_dims, latent_dim, dropout_rate)
criterion = MaskedMSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Add weight_decay

# Move model to GPU if available

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
train_matrix = train_matrix.to(device)
train_mask = train_mask.to(device)


#  Training Loop ---
print("\n--- Training Enhanced Autoencoder ---")
for epoch in range(num_epochs):
    model.train() # Set model to training mode
    total_loss = 0
    for batch_matrix, batch_mask in train_loader:
        batch_matrix, batch_mask = batch_matrix.to(device), batch_mask.to(device)

        optimizer.zero_grad() # Clear gradients

        # Forward pass
        outputs = model(batch_matrix)

        # Calculate loss using the mask
        loss = criterion(outputs, batch_matrix, batch_mask)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

print("\nEnhanced Autoencoder training complete.")

## Evaluation and Comparison
#  Prediction and Evaluation ---

model.eval() # Set model to evaluation mode

# Get reconstructed matrix (contains predicted ratings for all items for all users)
# We need to pass the *entire* train_matrix_tensor through the model to get all predictions
with torch.no_grad(): # Disable gradient calculation for inference
    full_reconstructed_matrix = model(train_matrix.to(device)).cpu().numpy() # Process the whole matrix

# Convert Surprise testset to a format usable for evaluation
test_predictions = []
for uid, iid, true_r in testset:
    # Get predicted rating from the reconstructed matrix
    # Ensure uid and iid are within the bounds of full_reconstructed_matrix
    if uid < full_reconstructed_matrix.shape[0] and iid < full_reconstructed_matrix.shape[1]:
        predicted_r = full_reconstructed_matrix[uid, iid]
    else:
        # Handle cases where uid/iid from testset might be out of bounds if data handling was inconsistent
        # (Shouldn't happen if n_users/n_items are from df.nunique())
        predicted_r = reader.rating_scale[0] # Default to min rating or average if out of bounds

    test_predictions.append(Prediction(uid, iid, true_r, predicted_r, None))

# Evaluate with Surprise's accuracy metrics
print("\n--- Enhanced Autoencoder Model Evaluation ---")
ae_rmse = accuracy.rmse(test_predictions, verbose=False)
ae_mse = accuracy.mse(test_predictions, verbose=False)
ae_mae = accuracy.mae(test_predictions, verbose=False)
ae_fcp = accuracy.fcp(test_predictions, verbose=False)

print(f"Autoencoder RMSE: {ae_rmse:.4f}")
print(f"Autoencoder MSE: {ae_mse:.4f}")
print(f"Autoencoder MAE: {ae_mae:.4f}")
print(f"Autoencoder FCP: {ae_fcp:.4f}")


#  Add Autoencoder to Comparison Table and Plot ---

metrics_df.index.name = 'Algorithm'

# Add Enhanced Autoencoder results
ae_results = {'RMSE': ae_rmse, 'MSE': ae_mse, 'MAE': ae_mae, 'FCP': ae_fcp}
metrics_df.loc['EnhancedAutoencoder'] = ae_results

print("\n--- Updated Comparison of Algorithm Metrics (with Enhanced Autoencoder) ---")
print(metrics_df)

"""Observations:

1. High Error Metrics: Both Autoencoder and Enhanced Autoencoder have significantly higher RMSE, MSE, and MAE values compared to all other models. This indicates that their predicted ratings are much further away from the actual ratings.
2. Enhanced Autoencoder is Worse: Counter-intuitively, the "Enhanced" Autoencoder (with more layers, LeakyReLU, Dropout, and L2 regularization) performs even worse than the simpler Autoencoder. This is a strong indicator of either severe overfitting or instability during training due to the increased complexity on a dataset that cannot support it.
3. FCP Anomaly: While the error metrics are bad, the basic Autoencoder's FCP (0.544389) is actually the highest among all models, even surpassing KNNBasic. This is an interesting, though likely misleading, result. It might suggest that while the magnitude of its predictions is way off, its relative ordering for some pairs might coincidentally be better, but given the very high RMSE, it's unlikely to be useful. The Enhanced Autoencoder's FCP is lower than the basic one and most other models.

Why Autoencoders (especially DNNs) Might Perform Poorly
1. Extreme Data Sparsity (Most Likely Cause for Your Data)
2. Small Dataset Size
3. Hyperparameter Misconfiguration (Very Common)
4. Loss Function Interaction with Sparsity

How to improve
1. Use a Larger, More Realistic Dataset
2. Aggressive Hyperparameter Tuning
3. Consider Different Autoencoder Variants:

  3.1 **Variational Autoencoders (VAEs)**: These are specifically designed for collaborative filtering and often perform much better on sparse data by learning a probabilistic latent space.
  
  3.2 **Denoising Autoencoders (DAEs)**: They are trained to reconstruct a clean input from a corrupted version, which can make them more robust to noise and sparsity.
  
  3.3 **Bias-Regularized Autoencoders**: Explicitly incorporating user/item biases into the autoencoder architecture or loss function.

#### Plot and Inference
"""

# --- Plotly Chart (updated to include Enhanced Autoencoder) ---
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=2,
                    subplot_titles=("Error Metrics (Lower is Better)", "Ranking Metric (FCP - Higher is Better)"))

fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['RMSE'], name='RMSE', marker_color='skyblue', hovertemplate='<b>%{x}</b><br>RMSE: %{y:.4f}<extra></extra>'), row=1, col=1)
fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['MAE'], name='MAE', marker_color='lightcoral', hovertemplate='<b>%{x}</b><br>MAE: %{y:.4f}<extra></extra>'), row=1, col=1)
fig.add_trace(go.Bar(x=metrics_df.index, y=metrics_df['FCP'], name='FCP', marker_color='lightgreen', hovertemplate='<b>%{x}</b><br>FCP: %{y:.4f}<extra></extra>'), row=1, col=2)

fig.update_layout(
    title_text='Comparison of Recommendation Algorithm Performance',
    height=600, # Adjust height for more models
    showlegend=True,
    barmode='group',
    hovermode='x unified'
)

fig.update_yaxes(title_text="Error Value", row=1, col=1)
fig.update_yaxes(title_text="FCP Value", row=1, col=2)

fig.show()

# --- Sample User Prediction with Autoencoder ---
print("\n--- Generating Sample Predictions for a User (Enhanced Autoencoder) ---")

sample_reviewer_original = df['Reviewer'].iloc[0] # Using original ID for display
sample_user_id_encoded = df.loc[df['Reviewer'] == sample_reviewer_original, 'user_id'].iloc[0]

print(f"Sample User: '{sample_reviewer_original}' (encoded as user_id: {sample_user_id_encoded})")

# Get items the user has NOT rated in the trainset
all_item_ids = df['item_id'].unique()
user_rated_items_trainset = set()
for (item_inner_id, rating) in trainset.ur[sample_user_id_encoded]:
    user_rated_items_trainset.add(item_inner_id)

unrated_item_ids = [iid for iid in all_item_ids if iid not in user_rated_items_trainset]

item_id_to_name = df.set_index('item_id')['Car Model'].drop_duplicates().to_dict()

if len(unrated_item_ids) == 0:
    print("This user has rated all available items in the training set or no unrated items found for prediction.")
else:
    items_to_predict_for_ae = unrated_item_ids[:min(len(unrated_item_ids), 5)] # Predict for up to 5 unrated items

    if items_to_predict_for_ae:
        print(f"\nTop {len(items_to_predict_for_ae)} predicted ratings for user '{sample_reviewer_original}' (using Enhanced Autoencoder model):")
        predictions_for_user_ae = []
        for item_id_encoded in items_to_predict_for_ae:
            # Predictions come directly from the full_reconstructed_matrix
            predicted_rating = full_reconstructed_matrix[sample_user_id_encoded, item_id_encoded]
            predictions_for_user_ae.append((item_id_to_name.get(item_id_encoded, f"Unknown Item ({item_id_encoded})"), predicted_rating))

        predictions_for_user_ae.sort(key=lambda x: x[1], reverse=True)

        for item_name, pred_rating in predictions_for_user_ae:
            print(f"  - Car Model: {item_name}, Predicted Rating: {pred_rating:.2f}")
    else:
        print("No unrated items available for prediction.")

"""### BPR

#### Data Pred for the model
"""

from lightfm.data import Dataset

# --- 1. Data Preprocessing for LightFM BPR ---

# LightFM requires user and item IDs to be in a specific format (integer IDs)
# and for the interaction matrix to be a sparse matrix.
# The `Dataset` class in LightFM helps with this mapping and matrix creation.

# Ensure data is clean (no NaNs or empty strings that might cause issues for LightFM's Dataset)
df_clean = df.dropna(subset=['Reviewer', 'Car Model']).copy()
df_clean = df_clean[df_clean['Reviewer'] != ''].copy()
df_clean = df_clean[df_clean['Car Model'] != ''].copy()


dataset = Dataset()
dataset.fit(
    users=df_clean['Reviewer'].unique(),
    items=df_clean['Car Model'].unique()
)

num_users, num_items = dataset.interactions_shape()
print(f'Number of users: {num_users}, Number of items: {num_items}.')

(interactions, weights) = dataset.build_interactions(
    (row['Reviewer'], row['Car Model'], row['Rating']) for index, row in df_clean.iterrows()
)

print("Interaction matrix shape:", interactions.shape)
print("Interaction matrix type:", type(interactions))

# --- CRITICAL MAPPINGS UNPACK AND VERIFICATION ---
all_mappings = dataset.mapping()

user_string_to_internal_id = all_mappings[0]
user_internal_id_to_string = all_mappings[1]
item_string_to_internal_id = all_mappings[2]
# The problematic variable, which you confirmed is str -> int
problematic_item_internal_id_to_string_var = all_mappings[3]

# FIX: Manually invert the problematic dictionary to get int -> str mapping
# This assumes problematic_item_internal_id_to_string_var indeed contains str -> int
# and that this is the correct set of mappings.
item_internal_id_to_string_correct = {v: k for k, v in problematic_item_internal_id_to_string_var.items()}


print("\n--- LightFM Mappings Detailed Debug Info (Post-Fix) ---")
print(f"**Original variable from mapping (item_string_to_internal_id, as confirmed by your output)**")
print(f"Type: {type(item_string_to_internal_id)}")
print(f"Example (first 2 items): {list(item_string_to_internal_id.items())[:2]}")
print(f"Does it contain key 'Lincoln Mks (2014)'? {'Lincoln Mks (2014)' in item_string_to_internal_id}")
print(f"Does it contain key 0? {0 in item_string_to_internal_id} (Expected False)")

print("\n**Corrected variable (item_internal_id_to_string_correct - should be int keys, string values)**")
print(f"Type: {type(item_internal_id_to_string_correct)}")
if num_items > 0:
    print(f"Example (first 2 items): {list(item_internal_id_to_string_correct.items())[:min(2, len(item_internal_id_to_string_correct))]}")
print(f"Does it contain key 0? {0 in item_internal_id_to_string_correct} (Expected True if num_items > 0)")
if 0 in item_internal_id_to_string_correct:
    print(f"Internal ID 0 maps to: {item_internal_id_to_string_correct[0]}")
else:
    print("CRITICAL WARNING: Internal ID 0 is NOT found in item_internal_id_to_string_correct. This is highly unexpected even with inversion, indicating deeper data/environment issues.")
print("--- End Detailed Debug Info ---\n")

"""#### Model Training and Evaluating"""

# If you wanted to specify weights (e.g., higher ratings get higher weights), you could use the 'weights' matrix.
# For standard BPR, you typically just care about the presence of an interaction, so the `weights` matrix
# is often uniform (or ignored if loss='bpr' is used and not explicit weights are passed during fit).
# LightFM handles the implicit nature of BPR based on the presence of entries in the interaction matrix.

# --- 2. Build and Train the LightFM BPR Model ---

# Initialize the LightFM model with BPR loss
# no_components: Dimensionality of the latent factors
# loss='bpr': Specifies Bayesian Personalized Ranking loss
# learning_rate: Controls how quickly the model updates its parameters
# item_alpha: L2 regularization on item embeddings
# user_alpha: L2 regularization on user embeddings
# random_state: For reproducibility
model = LightFM(
    no_components=30, # A common choice for latent dimensionality
    loss='bpr',       # Bayesian Personalized Ranking loss
    learning_rate=0.03,
    item_alpha=1e-6,  # Regularization to prevent overfitting
    user_alpha=1e-6,
    random_state=42
)

print("\n--- Training LightFM BPR Model ---")
# Train the model
# epochs: Number of passes over the training data
# num_threads: Number of threads to use for training (for faster computation)
model.fit(interactions, epochs=100, num_threads=4, verbose=True) # verbose=True to see training progress

print("LightFM BPR training complete.")

# --- 3. Evaluate the Model (Optional but Recommended) ---
# For ranking models like BPR, AUC and Precision@K are more appropriate than RMSE/MAE

train_auc = auc_score(model, interactions, num_threads=4).mean()
print(f"Train AUC score: {train_auc:.4f}")

"""#### Sample Recommendataion"""

# --- 4. Generate Recommendations for a Sample User ---
print("\n--- Generating Sample Recommendations with LightFM BPR ---")

sample_reviewer_original = df_clean['Reviewer'].iloc[0]
sample_user_id_internal = user_string_to_internal_id[sample_reviewer_original]

print(f"Sample User: '{sample_reviewer_original}' (LightFM internal ID: {sample_user_id_internal})")

user_rated_items_original = df_clean[df_clean['Reviewer'] == sample_reviewer_original]['Car Model'].unique()
print(f"Items '{sample_reviewer_original}' has interacted with: {list(user_rated_items_original)}")

all_item_ids_internal = np.arange(num_items)

predictions = model.predict(
    user_ids=sample_user_id_internal,
    item_ids=all_item_ids_internal
)

# Convert item internal IDs back to original names and get scores
item_scores = []
for item_id_internal, score in zip(all_item_ids_internal, predictions):
    # Use the corrected dictionary here
    original_item_name = item_internal_id_to_string_correct[item_id_internal]
    item_scores.append((original_item_name, score))

# Filter out items the user has already seen and sort by score
recommended_items = []
for item_name, score in item_scores:
    if item_name not in user_rated_items_original:
        recommended_items.append((item_name, score))

# Sort recommendations by predicted score in descending order
recommended_items.sort(key=lambda x: x[1], reverse=True)

print(f"\nTop 5 Recommended Car Models for '{sample_reviewer_original}' (using LightFM BPR):")
if not recommended_items:
    print("No recommendations found (user may have interacted with all items, or very few unseen items).")
else:
    for i, (item_name, score) in enumerate(recommended_items[:5]):
        print(f"  {i+1}. Car Model: {item_name}, Predicted Score: {score:.4f}")

print("\nNote: LightFM BPR predicts a preference score (higher is better), not a rating scale (1-5).")
print("The primary goal is to rank items correctly for the user.")



